### :purple_heart: Hey there :purple_heart:

I am a final year PhD student of Statistics :game_die: at London School of Economics. 

- ðŸ”­ Iâ€™m currently working on deep generative models.  

- ðŸŒ± Iâ€™m interested in teaching machines how to see :eyes:, speak :mega:, sing :musical_score:, draw :art:, think :brain: and act :running:.  

- :e-mail: How to reach me: tianlin.xu1@gmail.com

#### :scroll: Publications | Preprints :scroll:
##### [COT-GAN: Generating Sequential Data via Causal Optimal Transport.](https://papers.nips.cc/paper/2020/file/641d77dd5271fca28764612a028d9c8e-Paper.pdf) Xu T., Wenliang L., Munn M, & Acciaio B. (2020). Conference on Neural Information Processing Systems(NeurIPS).

- :speech_balloon: **Introduction**: COT-GAN is an adversarial algorithm to train implicit generative models. In addition to simply using recurrent network structure to model sequential data,  the contribution of COT-GAN is to propose an objective function, formulated using ideas from Causal Optimal Transport (COT), which naturally encodes an additional temporal causality constaint. This objective function is optimised for learning time dependent data distributions, and generic for most applications of sequential nature, e.g., video, music, speech, stock prices and so on.  
- :computer: **Code**: [TensorFlow 2.x verson](https://github.com/tianlinxu312/cot-gan) and [PyTorch version](https://github.com/tianlinxu312/cot-gan-pytorch)

##### [Double Generative Adversarial Networks for Conditional Independence Testing.](https://arxiv.org/pdf/2006.02615.pdf) Shi C., Xu T., Bergsma W, & Li L. (2020). Under review.

- :speech_balloon: **Introduction**: The contributions of this paper involves two key components:  1. we construct a doubly-robust test statistic which offers additional protections against potential misspecification of the conditional distributions, 2. we propose a double GAN-based inference procedure for the conditional independence testing problem.

- :computer: **Code**: https://github.com/tianlinxu312/dgcit

##### [Variational f-divergence Minimization.](https://arxiv.org/pdf/1907.11891.pdf) Zhang M., Bird T., Habib R., Xu T., & Barber D. (2019). Conference on Neural Information Processing Systems(NeurIPS) workshop.

- :speech_balloon: **Introduction**: Probabilistic models are often trained by maximum likelihood, which corresponds to minimizing a specific f-divergence (forward KL divergence) between the model and data distribution. A general variational method for training latent variable models using maximum likelihood is well established; however, how to train latent variable models using other f-divergences is comparatively unknown. The contribution of this paper is the derivation of a generic variantional upper bound that can be applied to train a large class of latent variable models using any f-divergence.

<!--
**tianlinxu312/tianlinxu312** is a âœ¨ _special_ âœ¨ repository because its `README.md` (this file) appears on your GitHub profile.

- :mortar_board: Google Scholar: https://scholar.google.com/citations?user=KPrpfPsAAAAJ&hl=en

- ðŸ”­ Iâ€™m currently working on ...
- ðŸŒ± Iâ€™m currently learning ...
- ðŸ‘¯ Iâ€™m looking to collaborate on ...
- ðŸ¤” Iâ€™m looking for help with ...
- ðŸ’¬ Ask me about ...
- ðŸ“« How to reach me: ...
- ðŸ˜„ Pronouns: ...
- âš¡ Fun fact: ...
-->
